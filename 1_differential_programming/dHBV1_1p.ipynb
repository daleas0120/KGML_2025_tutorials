{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6844a39c",
      "metadata": {
        "id": "6844a39c"
      },
      "source": [
        "## **Differentiable parameter learning (dPL) + HBV hydrologic model**\n",
        "\n",
        "## By Multi-scale Hydrology, Processes and Intelligence (MHPI) team from The Pennsylvania State University\n",
        "## Dr. Chaopeng Shen's Group in Hydrologic Deep Learning and Modeling\n",
        "\n",
        "## **Citations:**\n",
        "\n",
        "dHBV1.0: Feng, Dapeng, Liu, Jiangtao, Lawson, Kathryn., & Shen, Chaopeng (2022). Differentiable, learnable, regionalized process-based models with multiphysical outputs can approach state-of-the-art hydrologic prediction accuracy. Water Resources Research, 58, e2022WR032404. https://doi.org/10.1029/2022WR032404\n",
        "\n",
        "dHBV1.1p: Yalan Song, Kamlesh Sawadekar, Jonathan M Frame, et al. Physics-informed, Differentiable Hydrologic  Models for Capturing Unseen Extreme Events  . ESS Open Archive . March 14, 2025. https://doi.org/10.22541/essoar.172304428.82707157/v2\n",
        "\n",
        "This script is prepared by Yalan Song and managed by MHPI group.\n",
        "\n",
        "### **Original Code Release**\n",
        "dHBV1.0: Feng, Dapeng, Shen, Chaopeng, Liu, Jiangtao, Lawson, Kathryn, & Beck, Hylke. (2022). differentiable parameter learning (dPL) + HBV hydrologic model. Zenodo. https://doi.org/10.5281/zenodo.7091334\n",
        "\n",
        "dHBV1.1p: Song, Y., Feng, D., & Shen, C. (2025). Differentiable hydrologic model (ùõøHBV1.1p). Zenodo. https://doi.org/10.5281/zenodo.15978037"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfe46df5",
      "metadata": {
        "id": "bfe46df5"
      },
      "outputs": [],
      "source": [
        "### Dear Code User,\n",
        "\n",
        "### Thank you for using our dPL model. We are glad to see our code being\n",
        "### used to advance research in our field.\n",
        "\n",
        "### As you use our code, we kindly request that you review and cite\n",
        "### the relevant papers above that were used to develop the code.\n",
        "### By doing so, you will help ensure that the contributions of\n",
        "### the researchers who developed the underlying algorithms\n",
        "### are properly recognized and appreciated.\n",
        "\n",
        "### We appreciate your cooperation in this matter and would be happy to\n",
        "### assist you in finding the appropriate sources to cite.\n",
        "### If you have any questions or concerns, please do not hesitate to contact us.\n",
        "\n",
        "### Thank you for your support!\n",
        "### If you have any question for this release, please contact Dapeng Feng(duf328@psu.edu), Yalan Song (yxs275@psu.edu), or Chaopeng Shen(cshen@engr.psu.edu)\n",
        "\n",
        "### The code of the differentiable model can be downloaded at https://doi.org/10.5281/zenodo.7091334 or https://github.com/mhpi/dPLHBVrelease\n",
        "### Reference papers:\n",
        "### Feng, Dapeng, Jiangtao Liu, Kathryn Lawson, and Chaopeng Shen.\"Differentiable, Learnable, Regionalized Process‚ÄêBased Models With Multiphysical Outputs can Approach State‚ÄêOf‚ÄêThe‚ÄêArt Hydrologic Prediction Accuracy.\" Water Resources Research 58, no. 10 (2022): e2022WR032404.\n",
        "### Feng, Dapeng, Hylke Beck, Kathryn Lawson, and Chaopeng Shen. \"The suitability of differentiable, learnable hydrologic models for ungauged regions and climate change impact assessment.\" Hydrology and Earth System Sciences (2023). doi: 10.5194/hess-27-2357-2023\n",
        "### Yalan Song, Kamlesh Sawadekar, Jonathan M Frame, et al. Physics-informed, Differentiable Hydrologic  Models for Capturing Unseen Extreme Events  . ESS Open Archive . March 14, 2025. https://doi.org/10.22541/essoar.172304428.82707157/v2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Request GPU on google collab:\n",
        "### Click on the \"Runtime\" menu at the top of the page, and select \"Change runtime type\".\n",
        "\n",
        "### In the \"Hardware accelerator\" dropdown menu, select \"GPU\" and click \"Save\".\n",
        "\n",
        "### Colab will now restart your runtime and you should now have access to a GPU. You can check that the GPU is available by running the following code:\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "fMI4j87Bv-lf"
      },
      "id": "fMI4j87Bv-lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "rootDatabase = os.getcwd()\n",
        "print(rootDatabase)\n"
      ],
      "metadata": {
        "id": "uFDdyG3Ew0KA"
      },
      "id": "uFDdyG3Ew0KA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Zenodo link\n",
        "zenodo_url = \"https://zenodo.org/records/15978037/files/dPLHBVrelease-master.zip?download=1\"\n",
        "\n",
        "# Download the file\n",
        "local_zip_path = \"dPLHBVrelease.zip\"  # Change the file name if needed\n",
        "wget.download(zenodo_url, local_zip_path)\n",
        "\n",
        "# Extract the ZIP file\n",
        "extracted_folder_path = rootDatabase+\"/dPLHBVrelease\"\n",
        "with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "# Clean up the ZIP file after extraction\n",
        "os.remove(local_zip_path)\n",
        "\n",
        "print(f\"Files downloaded and extracted to {extracted_folder_path}\")"
      ],
      "metadata": {
        "id": "CvBirl8_xQeR"
      },
      "id": "CvBirl8_xQeR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "02c0e512",
      "metadata": {
        "id": "02c0e512"
      },
      "source": [
        "# The following code is to prepare the input and target data from CAMELS dataset for differentiable hydrologic models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c613a5f",
      "metadata": {
        "id": "3c613a5f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Run this code to download CAMELS data locally and get training_file/validation_file which are pickle files containing numpy floats\n",
        "change rootDatabase = r\"E:\\Downloads\\CAMELS\" below to your download directory. This code works on Linux or Windows.\n",
        "Afterwards, you can simply use the pickle files (upload to colab if running on colab) defined inline below, e.g.,\n",
        "train_file = 'training_file' # contains train_x, train_y, train_c as tensors\n",
        "validation_file = 'validation_file' # contains val_x, val_y, val_c\n",
        "These variables have not been normalized, and are numpy ndarray as follows:\n",
        "train_x (forcing data, e.g. precipitation, temperature ...): [pixels, time, features]\n",
        "train_c (constant data, e.g. soil properties, land cover ...): [pixels, features]\n",
        "train_y (target variable, e.g. soil moisture, streamflow ...): [pixels, time, 1]\n",
        "val_x, val_c, val_y\n",
        "The variables to download, training and test time periods, etc., are defined in the last block of this file. Customize as you wish\n",
        "\n",
        "\n",
        "\n",
        "**simple directions to get set up with Python on your local computer and run this script:\n",
        "Download/install Miniconda (mini version of Python package manager, Anaconda)\n",
        "save this script file in a new folder\n",
        "optional** download file from https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip and save to folder (otherwise will autodownload via the script but may fail)\n",
        "open anaconda prompt (miniconda)\n",
        "navigate to the folder with your script: cd prints current directory, dir prints directory contents, cd foldername moves your current directory into the named folder\n",
        "\n",
        "then input the following commands into the command line interface (anaconda prompt):\n",
        "conda create -n test_hydrodl_tutorial python=3.9 ##Important: We are specifying Python 3.9 due to a number of code dependencies and packages that are not yet fully up to date with the most recent versions of python\n",
        "#(confirm creation with y)\n",
        "conda activate test_hydrodl_tutorial\n",
        "conda install requests tqdm pytorch pandas\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d42df2",
      "metadata": {
        "id": "21d42df2"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "import os\n",
        "\n",
        "target_path = rootDatabase+ '/dPLHBVrelease/dPLHBVrelease-master/dPLHBVrelease-master/hydroDL-dev'\n",
        "\n",
        "sys.path.append(str(target_path))  # Convert Path object to string before appending\n",
        "\n",
        "print(\"Model path:\", target_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b4edb5",
      "metadata": {
        "id": "59b4edb5"
      },
      "outputs": [],
      "source": [
        "# If you have your CAMELS data downloaded in the path you provided above, you can skip this step!\n",
        "import platform\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "\n",
        "def download_file(url, destination):\n",
        "    \"\"\"Download a file from a specified URL to a given destination.\"\"\"\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(destination, 'wb') as file:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            file.write(chunk)\n",
        "\n",
        "def downloadCAMELS():\n",
        "  if platform.system() == 'Windows':\n",
        "\n",
        "    def unzip_file(source, destination):\n",
        "        \"\"\"Unzip a file from a source to a destination.\"\"\"\n",
        "        with zipfile.ZipFile(source, 'r') as zip_ref:\n",
        "            zip_ref.extractall(destination)\n",
        "\n",
        "    # Base directory\n",
        "    base_dir = os.getcwd()\n",
        "\n",
        "    # Create necessary directories\n",
        "    os.makedirs(base_dir + 'camels_attributes_v2.0/', exist_ok=True)\n",
        "    os.makedirs(base_dir + 'camels_attributes_v2.0/camels_attributes_v2.0/', exist_ok=True)\n",
        "\n",
        "    # Download and unzip the main dataset\n",
        "    main_dataset_url = 'https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    main_dataset_dest = base_dir + 'basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    download_file(main_dataset_url, main_dataset_dest)\n",
        "    unzip_file(main_dataset_dest, base_dir + 'basin_timeseries_v1p2_metForcing_obsFlow/')\n",
        "\n",
        "    ### Download potential evapotranspiration data\n",
        "    download_file('https://zenodo.org/record/7943626/files/pet_harg.zip?download=1',base_dir + 'pet_harg.zip')\n",
        "    unzip_file(base_dir + 'pet_harg.zip', base_dir)\n",
        "\n",
        "\n",
        "    # List of other files to download\n",
        "    files_to_download = {\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.xlsx': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_attributes_v2.0.xlsx',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_clim.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_geol.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_hydro.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_name.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_name.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_topo.txt',\n",
        "        'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt': 'camels_attributes_v2.0/camels_attributes_v2.0/camels_vege.txt'\n",
        "    }\n",
        "\n",
        "    # Download additional files\n",
        "    for url, dest in files_to_download.items():\n",
        "        full_dest = os.path.join(base_dir, dest)\n",
        "        download_file(url, full_dest)\n",
        "\n",
        "    print(\"Download and extraction complete.\")\n",
        "\n",
        "# Add additional download and unzip commands as needed\n",
        "\n",
        "  else:\n",
        "    base_dir = os.getcwd()\n",
        "    print(\"CAMELS data is downloading to \",base_dir)\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/basin_timeseries_v1p2_metForcing_obsFlow.zip' -O {base_dir}'/basin_timeseries_v1p2_metForcing_obsFlow.zip'\n",
        "    !unzip -o {base_dir}'/basin_timeseries_v1p2_metForcing_obsFlow.zip' -d {base_dir}'/basin_timeseries_v1p2_metForcing_obsFlow'\n",
        "    !mkdir {base_dir}'/camels_attributes_v2.0/'\n",
        "    !mkdir {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.xlsx' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_attributes_v2.0.xlsx'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_clim.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_clim.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_geol.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_geol.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_hydro.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_hydro.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_name.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_name.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_topo.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_topo.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_soil.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_soil.txt'\n",
        "    !wget 'https://gdex.ucar.edu/dataset/camels/file/camels_vege.txt' -O {base_dir}'/camels_attributes_v2.0/camels_attributes_v2.0/camels_vege.txt'\n",
        "\n",
        "    ### Download potential evapotranspiration data\n",
        "    !wget 'https://zenodo.org/record/7943626/files/pet_harg.zip?download=1' -O {base_dir}'/pet_harg.zip'\n",
        "    !unzip -o  {base_dir}'/pet_harg.zip' -d {base_dir}\n",
        "\n",
        "# caution: long-time needed\n",
        "# you shouldn't need to run this if you are loading directly from pickle file\n",
        "downloadCAMELS()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d9fdd37",
      "metadata": {
        "id": "8d9fdd37"
      },
      "outputs": [],
      "source": [
        "# DATA extraction function\n",
        "from hydroDL import utils\n",
        "import numpy as np\n",
        "import pickle\n",
        "def extractCAMELS(Ttrain,attrLst,varF,camels,forType='daymet',subset_train=\"All\",subset_idx = None,file_path=None):\n",
        "\n",
        "  train_loader = camels.DataframeCamels(subset=subset_train, tRange=Ttrain, forType=forType)\n",
        "  x = train_loader.getDataTs(varLst=varF, doNorm=False, rmNan=False)\n",
        "  y = train_loader.getDataObs(doNorm=False, rmNan=False, basinnorm=False)\n",
        "  c = train_loader.getDataConst(varLst=attrLst, doNorm=False, rmNan=False)\n",
        "\n",
        "\n",
        "  # Reading prepared PET data\n",
        "  # Modify this as the directory where you put PET\n",
        "  PETDir = camels.dirDB + '/pet_harg/' + forType + '/'\n",
        "  if subset_train==\"All\":\n",
        "     usgsIdLst = camels.gageDict['id']\n",
        "  else:\n",
        "     usgsIdLst =  subset_train\n",
        "  if forType == 'maurer':\n",
        "      tPETRange = [19800101, 20090101]\n",
        "  else:\n",
        "      tPETRange = [19800101, 20150101]\n",
        "  tPETLst = utils.time.tRange2Array(tPETRange)\n",
        "  ntime = len(tPETLst)\n",
        "\n",
        "\n",
        "  PETfull = np.empty([len(usgsIdLst), ntime, 1])\n",
        "  for k in range(len(usgsIdLst)):\n",
        "\n",
        "      dataTemp = camels.readcsvGage(PETDir, usgsIdLst[k], ['PEVAP'], ntime)\n",
        "      PETfull[k, :, :] = dataTemp\n",
        "\n",
        "  TtrainLst = utils.time.tRange2Array(Ttrain)\n",
        "  C, ind1, ind2 = np.intersect1d(TtrainLst, tPETLst, return_indices=True)\n",
        "  PETUN = PETfull[:, ind2, :]\n",
        "  PETUN = PETUN[subset_idx, :, :]\n",
        "  x = np.concatenate([x, PETUN], axis=2)\n",
        "\n",
        "  # define dataset\n",
        "  if file_path is not None:\n",
        "    with open(file_path, 'wb') as f:\n",
        "      pickle.dump((x, y, c), f)\n",
        "  return x, y, c"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The following code can help you skip data downloading process"
      ],
      "metadata": {
        "id": "OfkS5EOm3NHp"
      },
      "id": "OfkS5EOm3NHp"
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1XVQ-15mIUJyuBOqkiOgiIbBgRR4ssvGD\n",
        "!gdown 1kEUAfTz13_C_UnPB2xQSle4SZnKhKrQ6"
      ],
      "metadata": {
        "id": "3P2Ja89z2pEW"
      },
      "id": "3P2Ja89z2pEW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2547d07",
      "metadata": {
        "id": "f2547d07"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import hydroDL\n",
        "from hydroDL.master import default\n",
        "from hydroDL.data import camels\n",
        "import numpy as np\n",
        "import json\n",
        "# If you already have data extracted in previous runs or you use gdown to skip downloading process, set this to True\n",
        "\n",
        "Justload = True\n",
        "\n",
        "forType = 'daymet'\n",
        "Ttrain = [19991001, 20081001] #training period\n",
        "valid_date = [19861001, 19991001]  # Testing period\n",
        "#define inputs\n",
        "if forType == 'daymet':\n",
        "  varF = [ 'prcp', 'tmean']\n",
        "else:\n",
        "  varF = [ 'prcp','tmax']\n",
        "\n",
        "train_file = 'training_file'\n",
        "validation_file = 'validation_file'\n",
        "# Define attributes list\n",
        "attrLst = [ 'p_mean','pet_mean', 'p_seasonality', 'frac_snow', 'aridity', 'high_prec_freq', 'high_prec_dur',\n",
        "            'low_prec_freq', 'low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
        "            'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
        "            'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
        "            'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
        "            'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
        "\n",
        "## Load all 671 basins from CAMELS\n",
        "# TrainLS =  camels.gageDict['id'].tolist()\n",
        "subsetPath = str(target_path) + '/example/dPLHBV/Sub531ID.txt'\n",
        "with open(subsetPath, 'r') as fp:\n",
        "     TrainLS = json.load(fp)\n",
        "TrainLS.sort()\n",
        "TrainInd = [TrainLS.index(j) for j in TrainLS]\n",
        "\n",
        "\n",
        "\n",
        "if not Justload:\n",
        "  camels.initcamels(rootDB=rootDatabase)\n",
        "  opt_data = default.optDataCamels # wrapping options around.\n",
        "  opt_data = default.update(opt_data, varT=varF, varC=attrLst, tRange=Ttrain, forType='forType', subset = TrainLS)\n",
        "  forcing_train, target_train, attr_train = extractCAMELS(Ttrain,attrLst,varF,camels,forType=forType,subset_train=TrainLS,subset_idx = TrainInd,file_path=train_file)\n",
        "  forcing_val, target_val, attr_val = extractCAMELS(valid_date,attrLst,varF,camels,forType=forType,subset_train=TrainLS,subset_idx = TrainInd,file_path=validation_file)\n",
        "  print(f\"written files to: \"+rootDatabase+os.path.sep+train_file+\" and \"+validation_file)\n",
        "else:\n",
        "  opt_data = default.optDataCamels # wrapping options around.\n",
        "  opt_data = default.update(opt_data, varT=varF, varC=attrLst, tRange=Ttrain, forType='forType', subset = TrainLS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc8d0cf",
      "metadata": {
        "id": "2fc8d0cf"
      },
      "source": [
        "### You want to know about the $\\delta$ HBV model\n",
        "#### The basic idea of the model is to use an LSTM to generate parameters‚Äîeither static or dynamic‚Äîfor the HBV hydrologic model.  \n",
        "#### The HBV model takes these physical parameters as inputs to simulate streamflow and other internal variables, such as ET, snow water equivalent, and baseflow.  \n",
        "#### The model can be summarized as:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\theta_d, \\theta_s &= \\mathrm{LSTM}(x_{\\text{norm}}, A_{\\text{norm}}) \\\\\n",
        "Q &= \\mathrm{HBV}(x, \\theta_d, \\theta_s)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "#### Where \\(x\\) represents the forcings used to drive the streamflow simulation, including precipitation, mean temperature, and potential evapotranspiration. \\(A\\) represents the static attributes listed above.\n",
        "#### Both \\(x\\) and \\(A\\) are concatenated and used as inputs to the LSTM.  Before input, \\(x\\) and \\(A\\) need to be normalized.\n",
        "#### The HBV model is a process-based model and uses the original (unnormalized) values of \\(x\\).\n",
        "#### $\\theta_d$ represents the dynamic parameters generated by the sequence-to-sequence LSTM, which change daily.  \n",
        "#### $\\theta_s$ represents the static parameters and uses the values from the last time step of the LSTM prediction.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23596f7b",
      "metadata": {
        "id": "23596f7b"
      },
      "source": [
        "### You want to know about HBV parameters:\n",
        "In $\\delta$ HBV1.0 (Feng et al., 2022), $\\beta$ (shape coefficient in the soil module) and $\\gamma$ (shape coefficient in the evapotranspiration module) are set as dynamic parameters.\n",
        "\n",
        "All HBV parameters are: parBETA, parFC, parK0, parK1, parK2, parLP, parPERC, parUZL, parTT, parCFMAX, parCFR, parCWH, parBETAET.\n",
        "\n",
        "The parameter indices of **$\\beta$** (`parBETA`) and **$\\gamma$** (`parBETAET`) are [1, 13]. All other parameters are set to static to avoid overfitting.\n",
        "\n",
        "**$\\beta$** (shape coefficient in the soil module) and **$\\gamma$** (shape coefficient in the evapotranspiration module) are set as dynamic parameters.\n",
        "\n",
        "All HBV parameters are: parBETA, parFC, parK0, parK1, parK2, parLP, parPERC, parUZL, parTT, parCFMAX, parCFR, parCWH, parBETAET\n",
        "\n",
        "The parameter indices of $\\beta$ (parBETA) and $\\gamma$ (parBETAET) are [1, 13].\n",
        "\n",
        "In $\\delta$ HBV1.1p (Song et al., 2025), All HBV parameters are: parBETA, parFC, parK0, parK1, parK2, parLP, parPERC, parUZL, parTT, parCFMAX, parCFR, parCWH, parBETAET, parC\n",
        "\n",
        "An additional parameter, `parC`, is introduced to represent upward flow in the HBV model, aiming to improve baseflow simulation.  \n",
        "\n",
        "More details about the HBV model can be found in the appendix of Song et al. (2025).\n",
        "\n",
        "For $\\delta$ HBV1.1p, we consider three options for dynamic parameter combinations:\n",
        "\n",
        "1. **$\\beta$** (`parBETA`) and **$\\gamma$** (`parBETAET`), indices **[1, 13]**\n",
        "\n",
        "2. **$\\beta$** (`parBETA`), **$K_0$** (`parK0`), and **$\\gamma$** (`parBETAET`), indices **[1, 3, 13]**  \n",
        "   *Recommended in Song et al. (2025); improves peak flow performance compared to option 1.*\n",
        "\n",
        "3. **$\\beta$** (`parBETA`), **$K_0$** (`parK0`), **$UZL$** (`parUZL`), and **$\\gamma$** (`parBETAET`), indices **[1, 3, 8, 13]**  \n",
        "   *Slight improvement over option 2, but adding more dynamic parameters requires caution to avoid overfitting or instability.*\n",
        "\n",
        "**$K_0$** and **$UZL$** are parameters that control the generation of near-surface flow (fast flow), and are related to time-dependent connectivity and variable source areas within the watershed.\n",
        "\n",
        "All other parameters are set to static to avoid overfitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299d2ce1",
      "metadata": {
        "id": "299d2ce1"
      },
      "source": [
        "## Let's start to train the differentiable model\n",
        "#### The following code is used to train $\\delta$ HBV1.1p  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d74d5e1",
      "metadata": {
        "id": "3d74d5e1"
      },
      "source": [
        "### Define Statistics Functions and a Scaler\n",
        "#### These functions are to normalize inputs (x and A) for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0289672d",
      "metadata": {
        "id": "0289672d"
      },
      "outputs": [],
      "source": [
        "# This part defines a hydrology-specific Scaler class that works similar as sklearn.MinMaxScaler\n",
        "# getStatDic, calcStat, calcStatgamma are supporting functions\n",
        "# If you want to use it with your own data, you can create your own scaler that can be used to normalize your data\n",
        "# Later on, we will use this scaler to transform the train and val data\n",
        "\n",
        "def getStatDic(log_norm_cols, attrLst=None, attrdata=None, seriesLst=None, seriesdata=None):\n",
        "  statDict = dict()\n",
        "  # series data\n",
        "  if seriesLst is not None:\n",
        "    for k in range(len(seriesLst)):\n",
        "      var = seriesLst[k]\n",
        "      if var in log_norm_cols:\n",
        "        statDict[var] = calcStatgamma(seriesdata[:, :, k])\n",
        "      else:\n",
        "        statDict[var] = calcStat(seriesdata[:, :, k])\n",
        "\n",
        "  # const attribute\n",
        "  if attrLst is not None:\n",
        "    for k in range(len(attrLst)):\n",
        "      var = attrLst[k]\n",
        "      statDict[var] = calcStat(attrdata[:, k])\n",
        "  return statDict\n",
        "\n",
        "def calcStat(x):\n",
        "  a = x.flatten()\n",
        "  b = a[~np.isnan(a)]\n",
        "  p10 = np.percentile(b, 10).astype(float)\n",
        "  p90 = np.percentile(b, 90).astype(float)\n",
        "  mean = np.mean(b).astype(float)\n",
        "  std = np.std(b).astype(float)\n",
        "  if std < 0.001:\n",
        "    std = 1\n",
        "  return [p10, p90, mean, std]\n",
        "\n",
        "def calcStatgamma(x):  # for daily streamflow and precipitation\n",
        "  a = x.flatten()\n",
        "  b = a[~np.isnan(a)]  # kick out Nan\n",
        "  b = np.log10(\n",
        "    np.sqrt(b) + 0.1\n",
        "  )  # do some tranformation to change gamma characteristics\n",
        "  p10 = np.percentile(b, 10).astype(float)\n",
        "  p90 = np.percentile(b, 90).astype(float)\n",
        "  mean = np.mean(b).astype(float)\n",
        "  std = np.std(b).astype(float)\n",
        "  if std < 0.001:\n",
        "    std = 1\n",
        "  return [p10, p90, mean, std]\n",
        "\n",
        "def transNormbyDic( x_in, var_lst, stat_dict, log_norm_cols, to_norm):\n",
        "  if type(var_lst) is str:\n",
        "    var_lst = [var_lst]\n",
        "  x = x_in.copy()\n",
        "  out = np.full(x.shape, np.nan)\n",
        "  for k in range(len(var_lst)):\n",
        "    var = var_lst[k]\n",
        "    stat = stat_dict[var]\n",
        "    if to_norm is True:\n",
        "      if len(x.shape) == 3:\n",
        "        if var in log_norm_cols:\n",
        "          x[:, :, k] = np.log10(np.sqrt(x[:, :, k]) + 0.1)\n",
        "        out[:, :, k] = (x[:, :, k] - stat[2]) / stat[3]\n",
        "      elif len(x.shape) == 2:\n",
        "        if var in log_norm_cols:\n",
        "          x[:, k] = np.log10(np.sqrt(x[:, k]) + 0.1)\n",
        "        out[:, k] = (x[:, k] - stat[2]) / stat[3]\n",
        "    else:\n",
        "      if len(x.shape) == 3:\n",
        "        out[:, :, k] = x[:, :, k] * stat[3] + stat[2]\n",
        "        if var in log_norm_cols:\n",
        "          out[:, :, k] = (np.power(10, out[:, :, k]) - 0.1) ** 2\n",
        "      elif len(x.shape) == 2:\n",
        "        out[:, k] = x[:, k] * stat[3] + stat[2]\n",
        "        if var in log_norm_cols:\n",
        "          out[:, k] = (np.power(10, out[:, k]) - 0.1) ** 2\n",
        "  return out\n",
        "\n",
        "\n",
        "\n",
        "class HydroScaler:\n",
        "  def __init__(self, attrLst, seriesLst,log_norm_cols):\n",
        "    self.log_norm_cols = log_norm_cols\n",
        "    self.attrLst = attrLst\n",
        "    self.seriesLst = seriesLst\n",
        "    self.stat_dict = None\n",
        "\n",
        "\n",
        "  def fit(self, attrdata, seriesdata):\n",
        "    self.stat_dict = getStatDic(\n",
        "      log_norm_cols=self.log_norm_cols,\n",
        "      attrLst=self.attrLst,\n",
        "      attrdata=attrdata,\n",
        "      seriesLst=self.seriesLst,\n",
        "      seriesdata=seriesdata,\n",
        "    )\n",
        "\n",
        "  def transform(self, data, var_list,):\n",
        "\n",
        "    norm_data = transNormbyDic(\n",
        "      data, var_list, self.stat_dict, log_norm_cols = self.log_norm_cols, to_norm=True)\n",
        "\n",
        "    return norm_data\n",
        "\n",
        "  def fit_transform(self, attrdata, seriesdata):\n",
        "    self.fit(attrdata, seriesdata)\n",
        "    attr_norm = self.transform(attrdata, self.attrLst)\n",
        "    series_norm = self.transform(seriesdata, self.seriesLst)\n",
        "    return attr_norm, series_norm\n",
        "\n",
        "\n",
        "# The function it only used for streamflow unit convert.\n",
        "def basinNorm(x, basinarea, toNorm):\n",
        "  nd = len(x.shape)\n",
        "  if nd == 3 and x.shape[2] == 1:\n",
        "    x = x[:, :, 0]  # unsqueeze the original 3 dimension matrix\n",
        "  temparea = np.tile(basinarea, (1, x.shape[1]))\n",
        "  if toNorm is True:\n",
        "    flow = (x * 0.0283168 * 3600 * 24) / (temparea * (10 ** 6)) * 10 ** 3  # ft^3/s --> mm/day\n",
        "  else:\n",
        "\n",
        "    flow = (\n",
        "            x\n",
        "            * ((temparea * (10**6)) * (10 ** (-3)))\n",
        "            / (0.0283168 * 3600 * 24)\n",
        "        )  # mm/day -->  ft^3/s\n",
        "  if nd == 3:\n",
        "    flow = np.expand_dims(flow, axis=2)\n",
        "  return flow\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2288b966",
      "metadata": {
        "id": "2288b966"
      },
      "source": [
        "### Initialize hydroDL Library, Set Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5be1190",
      "metadata": {
        "id": "c5be1190"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import time\n",
        "import os\n",
        "from tqdm import trange\n",
        "import random\n",
        "from hydroDL import master, utils\n",
        "from hydroDL.model import crit, train\n",
        "from hydroDL.model import rnn as rnn\n",
        "from hydroDL.data import camels\n",
        "from hydroDL.post import plot, stat\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "import datetime as dt\n",
        "\n",
        "\n",
        "traingpuid = 0\n",
        "torch.cuda.set_device(traingpuid)\n",
        "\n",
        "\n",
        "# Fix random seed\n",
        "seedid = 111111\n",
        "random.seed(seedid)\n",
        "torch.manual_seed(seedid)\n",
        "np.random.seed(seedid)\n",
        "torch.cuda.manual_seed(seedid)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set hyperparameters\n",
        "EPOCH = 100\n",
        "BATCH_SIZE = 100  # Each batch contrains 100 basins to train the LSTM\n",
        "RHO = 365   ## The time length of LSTM (RHO + BUFFTIME)\n",
        "HIDDENSIZE = 256 ## Hidden size of LSTM\n",
        "saveEPOCH = 10  # save model for every \"saveEPOCH\" epochs\n",
        "BUFFTIME = 365  ## The first 365 day is used to warmup (spin-up) the states of HBV model\n",
        "\n",
        "loadTrain = True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a79ac8",
      "metadata": {
        "id": "41a79ac8"
      },
      "source": [
        "### Please select your model option in the following cell\n",
        "### You can choose to run dHBV1.1p or dHBV1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49fac9b9",
      "metadata": {
        "id": "49fac9b9"
      },
      "outputs": [],
      "source": [
        "# Model option: \"dHBV1.0\" \"dHBV1.1p\"\n",
        "Model_option = \"dHBV1.1p\"\n",
        "# Dynamic paramter option : for \"dHBV1.0\", please use [1,13];  for \"dHBV1.1p\",  you can choose [1,13],[1,3,13] (Yalan recommends this option),[1,3,8, 13]\n",
        "# If you do not want any dynamic parameters: Set it to an empty list\n",
        "dyn_option = [1,3,13]\n",
        "\n",
        "# Number of components:\n",
        "# One component means a single set of HBV parameters;\n",
        "# 16 components mean 16 sets of HBV parameters, used to run 16 parallel HBV simulations ‚Äî these are then averaged before the routing step.\n",
        "\n",
        "# Note: Multiple components do not imply model ensembling using different random seeds.\n",
        "# We still use a single neural network to generate all sets of parameters.\n",
        "\n",
        "# Model performance ranking:\n",
        "# Static parameters with 1 component (1C) < Static parameters with 16 components (16C)  < Dynamic + static parameters with 16 components (16C)\n",
        "\n",
        "Nmul = 16\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model option: \"dHBV1.0\" \"dHBV1.1p\"\n",
        "Model_option = \"dHBV1.0\"\n",
        "# Dynamic paramter option : for \"dHBV1.0\", please use [1,13];  for \"dHBV1.1p\",  you can choose [1,13],[1,3,13] (Yalan recommends this option),[1,3,8, 13]\n",
        "# If you do not want any dynamic parameters: Set it to an empty list\n",
        "dyn_option = [1,13]\n",
        "\n",
        "# Number of components:\n",
        "# One component means a single set of HBV parameters;\n",
        "# 16 components mean 16 sets of HBV parameters, used to run 16 parallel HBV simulations ‚Äî these are then averaged before the routing step.\n",
        "\n",
        "# Note: Multiple components do not imply model ensembling using different random seeds.\n",
        "# We still use a single neural network to generate all sets of parameters.\n",
        "\n",
        "# Model performance ranking:\n",
        "# Static parameters with 1 component (1C) < Static parameters with 16 components (16C)  < Dynamic + static parameters with 16 components (16C)\n",
        "\n",
        "Nmul = 16\n"
      ],
      "metadata": {
        "id": "p5YQ80Cb1EhB"
      },
      "id": "p5YQ80Cb1EhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55a060fb",
      "metadata": {
        "id": "55a060fb"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dataset-specific code to read and scale the data\n",
        "IF YOU CAN DIRECTLY PROVIDE DATA for train_x, train_y, train_c, val_x, val_y, val_c and scale the data\n",
        "you can skip this section\n",
        "The data structure is as follows:\n",
        "train_x (forcing data, e.g. precipitation, temperature ...), shape: [pixels, time, features]\n",
        "train_c (constant data, e.g. soil properties, land cover ...), shape: [pixels, features]\n",
        "target/train_y (e.g. soil moisture, streamflow ...), shape: [pixels, time, 1]\n",
        "val_x, val_c, val_y\n",
        "Data type: numpy.float\n",
        "The Scaler does\n",
        "attr_norm, series_norm = scaler.fit_transform(train_c, series_data)\n",
        "val_c = scaler.transform(val_c, attrLst)\n",
        "val_x = scaler.transform(val_x, varF)\n",
        "\"\"\"\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Load X, Y, C from a file\n",
        "with open(train_file, 'rb') as f:\n",
        "    forcing_train, target_train, attr_train = pickle.load(f)  # Adjust this line based on your file format\n",
        "    print('Training forcing dimension:', forcing_train.shape, '(forcing data, e.g. precipitation, temperature ...): [basins, time, features]')\n",
        "    print('Training attributes dimension:', attr_train.shape, '(constant data, e.g. soil properties, land cover ...): [basins, features]')\n",
        "    print('Training target dimension:', target_train.shape, '(target variable, e.g. streamflow ...): [basins, time, 1]')\n",
        "with open(validation_file, 'rb') as g:\n",
        "    forcing_val, target_val, attr_val= pickle.load(g)  # Adjust this line based on your file format\n",
        "    print('Testing forcing dimension:', forcing_val.shape)\n",
        "    print('Testing attributes dimension:', attr_val.shape)\n",
        "    print('Testing target dimension:', target_val.shape)\n",
        "\n",
        "\n",
        "\n",
        "if forType == 'daymet':\n",
        "  varF = [ 'prcp', 'tmean']\n",
        "else:\n",
        "  varF = [ 'prcp','tmax']\n",
        "\n",
        "# Define attributes list\n",
        "attrLst = [ 'p_mean','pet_mean', 'p_seasonality', 'frac_snow', 'aridity', 'high_prec_freq', 'high_prec_dur',\n",
        "            'low_prec_freq', 'low_prec_dur', 'elev_mean', 'slope_mean', 'area_gages2', 'frac_forest', 'lai_max',\n",
        "            'lai_diff', 'gvf_max', 'gvf_diff', 'dom_land_cover_frac', 'dom_land_cover', 'root_depth_50',\n",
        "            'soil_depth_pelletier', 'soil_depth_statsgo', 'soil_porosity', 'soil_conductivity',\n",
        "            'max_water_content', 'sand_frac', 'silt_frac', 'clay_frac', 'geol_1st_class', 'glim_1st_class_frac',\n",
        "            'geol_2nd_class', 'glim_2nd_class_frac', 'carbonate_rocks_frac', 'geol_porostiy', 'geol_permeability']\n",
        "\n",
        "##Add PET in the time series list\n",
        "varF = varF + ['pet']\n",
        "# get a Scaler (similar to sklearn.MinMaxScaler).\n",
        "# basinNorm function is only for converting the streamflow unit from ft^3/s to mm/day.  The basin area are required. y_temp = train_y/(basinarea)\n",
        "# If your streamflow is not in the unit of ft^3/s, this function needs to be adapted\n",
        "basinarea  = attr_train[:,np.where(np.array(attrLst)=='area_gages2')[0]]\n",
        "\n",
        "##Convert the unit of streamflow to mm/day\n",
        "train_y = basinNorm(target_train,  basinarea, toNorm=True)\n",
        "\n",
        "val_y = basinNorm(target_val,  basinarea, toNorm=True)\n",
        "\n",
        "# Initialize scaler\n",
        "# This scaler will be used to scale the training data and later for the test as well.\n",
        "# Calculate the statistics for scaling the training data (x and A); Same statistics should be used for validation data for consistency.\n",
        "if Model_option == \"dHBV1.1p\":\n",
        "    log_norm_cols = []\n",
        "elif Model_option == \"dHBV1.0\":\n",
        "    log_norm_cols = ['prcp']\n",
        "else:\n",
        "    raise ValueError(\"No such model option. Please reselect it.\")\n",
        "\n",
        "scaler = HydroScaler(attrLst=attrLst, seriesLst=varF, log_norm_cols=log_norm_cols)\n",
        "\n",
        "\n",
        "# Fit and transform training data\n",
        "\n",
        "attr_norm, series_norm = scaler.fit_transform(attr_train, forcing_train.copy())\n",
        "attr_norm[np.isnan(attr_norm)] = 0.0   ## Nomalized attributes for LSTM\n",
        "\n",
        "train_x = forcing_train  # forcing data      ## Input forcing for the HBV model\n",
        "train_x[np.isnan(train_x)] = 0.0\n",
        "\n",
        "train_z = series_norm  # normalized forcing data      ## Nomalized forcing for LSTM\n",
        "train_z[np.isnan(train_z)] = 0.0\n",
        "\n",
        "train_c = attr_norm\n",
        "\n",
        "\n",
        "forcTuple_train = (train_x, train_z)\n",
        "\n",
        "# Fit and transform validation data\n",
        "\n",
        "val_c = scaler.transform(attr_val, attrLst)\n",
        "val_z = scaler.transform(forcing_val.copy(), varF)\n",
        "val_x = forcing_val\n",
        "# no need to scale val_y before we transform the prediction back to compare with val_y\n",
        "\n",
        "# fill some gaps in x. A fill for the normalized data is equal to filling with mean values\n",
        "# We don't fill y because it will be handled by the loss function\n",
        "val_c[np.isnan(val_c)] = 0.0\n",
        "val_z[np.isnan(val_z)] = 0.0\n",
        "val_x[np.isnan(val_x)] = 0.0\n",
        "\n",
        "\n",
        "\n",
        "#IF YOU CAN DIRECTLY PROVIDE DATA for train_x, train_y, train_c, val_x, val_y, val_c, and a Scaler and scale the data\n",
        "# you can skip this section\n",
        "#later code used scaler.stat_dict that matches with this scaler for testing. That could be changed for your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc0b1955",
      "metadata": {
        "id": "cc0b1955"
      },
      "outputs": [],
      "source": [
        "## Train the model\n",
        "# define loss function\n",
        "# dHBV1.1p uses a normalized sequare-error-based loss function\n",
        "if Model_option == \"dHBV1.1p\":\n",
        "    optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.NSELossBatch')\n",
        "    lossFun = crit.NSELossBatch(np.nanstd(train_y, axis=1))\n",
        "# dHBV1.0 uses a balances RMSE loss\n",
        "elif Model_option == \"dHBV1.0\":\n",
        "    alpha = 0.25 # a weight for RMSE loss to balance low and peak flow\n",
        "    optLoss = default.update(default.optLossComb, name='hydroDL.model.crit.RmseLossComb', weight=alpha)\n",
        "    lossFun = crit.RmseLossComb(alpha=alpha)\n",
        "else:\n",
        "    raise ValueError(\"No such model option. Please reselect it.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd63d57",
      "metadata": {
        "id": "bdd63d57"
      },
      "outputs": [],
      "source": [
        "TDOpt = True\n",
        "routing = True # Whether to use the routing module for simulated runoff\n",
        "comprout = False # True is doing routing for each component\n",
        "compwts = False # True is using weighted average for components; False is the simple mean\n",
        "pcorr = None # or a list to give the range of precip correction\n",
        "dydrop = 0.0 # dropout possibility for those dynamic parameters: 0.0 always dynamic; 1.0 always static\n",
        "staind = -1 # which time step to use from the learned para time series for those static parameters\n",
        "ETMod = True\n",
        "tdRep = dyn_option\n",
        "tdRepS = [str(ix) for ix in dyn_option]\n",
        "tdRepS_str = \"_\".join(tdRepS)\n",
        "if Model_option == \"dHBV1.1p\":\n",
        "    Nfea = 14\n",
        "# dHBV1.0 uses a balances RMSE loss\n",
        "elif Model_option == \"dHBV1.0\":\n",
        "    Nfea = 13\n",
        "else:\n",
        "    raise ValueError(\"No such model option. Please reselect it.\")\n",
        "\n",
        "\n",
        "\n",
        "rootOut = rootDatabase + f\"/dHBV1.1demo/\"\n",
        "if os.path.exists(rootOut) is False:\n",
        "    os.mkdir(rootOut)\n",
        "rootOut = rootOut + f\"/{Model_option}_{tdRepS_str}/\"\n",
        "\n",
        "if os.path.exists(rootOut) is False:\n",
        "    os.mkdir(rootOut)\n",
        "print('You model will saved in ',rootOut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62adc793",
      "metadata": {
        "id": "62adc793"
      },
      "outputs": [],
      "source": [
        "# define training options\n",
        "optTrain = default.update(default.optTrainCamels, miniBatch=[BATCH_SIZE, RHO], nEpoch=EPOCH, saveEpoch=saveEPOCH)\n",
        "# define output folder to save model results\n",
        "out = os.path.join(rootOut, f\"exp_EPOCH{EPOCH}_BS{BATCH_SIZE}_RHO{RHO}_HS{HIDDENSIZE}_trainBuff{BUFFTIME}_randomseed{seedid}\") # output folder to save results\n",
        "if os.path.exists(out) is False:\n",
        "    os.mkdir(out)\n",
        "print('You model will saved in ',out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d9826c",
      "metadata": {
        "id": "83d9826c"
      },
      "outputs": [],
      "source": [
        "# define and load model\n",
        "Ninv = val_x.shape[-1] + val_c.shape[-1]\n",
        "\n",
        "if Model_option == \"dHBV1.1p\":\n",
        "    modelname = 'HBV1_1p'\n",
        "# dHBV1.0 uses a balances RMSE loss\n",
        "elif Model_option == \"dHBV1.0\":\n",
        "    modelname = 'HBV1_0'\n",
        "else:\n",
        "    raise ValueError(\"No such model option. Please reselect it.\")\n",
        "\n",
        "model = rnn.dHBVModel(ninv=Ninv, nfea=Nfea, nmul=Nmul, hiddeninv=HIDDENSIZE, inittime=BUFFTIME,\n",
        "                                routOpt=routing, comprout=comprout, compwts=compwts, staind=staind, tdlst=tdRep,\n",
        "                                dydrop=dydrop, ETMod=ETMod,model_name = modelname)\n",
        "# dict only for logging\n",
        "optModel = OrderedDict(name= Model_option, nx=Ninv, nfea=Nfea, nmul=Nmul, hiddenSize=HIDDENSIZE, doReLU=True,\n",
        "                        Tinv=Ttrain, Trainbuff=BUFFTIME, routOpt=routing, comprout=comprout, compwts=compwts,\n",
        "                        pcorr=pcorr, staind=staind, tdlst=tdRep, dydrop=dydrop,buffOpt=0, TDOpt=TDOpt, ETMod=ETMod)\n",
        "# Wrap up all the training configurations to one dictionary in order to save into \"out\" folder as logging\n",
        "masterDict = master.wrapMaster(out, opt_data, optModel, optLoss, optTrain)\n",
        "master.writeMasterFile(masterDict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f805d9",
      "metadata": {
        "id": "24f805d9"
      },
      "outputs": [],
      "source": [
        "## Save the statistics\n",
        "statFile = os.path.join(out, 'statDict.json')\n",
        "with open(statFile, 'w') as fp:\n",
        "    json.dump(scaler.stat_dict, fp, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dbdd5b",
      "metadata": {
        "id": "54dbdd5b"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "\n",
        "trainedModel = train.trainModel(\n",
        "    model,\n",
        "    forcTuple_train,\n",
        "    train_y,\n",
        "    train_c,\n",
        "    lossFun,\n",
        "    nEpoch=EPOCH,\n",
        "    miniBatch=[BATCH_SIZE, RHO],\n",
        "    saveEpoch=saveEPOCH,\n",
        "    saveFolder=out,\n",
        "    bufftime=BUFFTIME)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "QX5TLOHbRp13"
      },
      "id": "QX5TLOHbRp13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "baa359b4",
      "metadata": {
        "id": "baa359b4"
      },
      "source": [
        "### The model training takes more than 8 hours with GPU for 531 basins and 9 years data.\n",
        "### Here we download a pre-rained model for testing to save  your time.\n",
        "\n",
        "## **Let us start to test the model now!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8849bd",
      "metadata": {
        "id": "aa8849bd"
      },
      "outputs": [],
      "source": [
        "if Model_option == \"dHBV1.1p\":\n",
        "    # Download the dHBV1.1p model from Google Drive\n",
        "    !gdown 1tWMIy243bxt-r6y4yLFsC8cDcHPTO4BE  # dHBV1.1p model file\n",
        "    # Copy the downloaded model to the output directory with a new name\n",
        "    !cp {rootDatabase}/dHBV1_1p_1_3_13_model_Ep100.pt {out}/model_Ep100.pt\n",
        "\n",
        "elif Model_option == \"dHBV1.0\":\n",
        "    # Download the dHBV1.0 model\n",
        "    !gdown 1qYUMDQyAZjjtwvRShYwLr4GGq5m6DSYC  # dHBV1.0 model file\n",
        "    # Copy and rename\n",
        "    !cp {rootDatabase}/dHBV1_0_1_13_model_Ep100.pt {out}/model_Ep100.pt\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"No such model option. Please reselect it.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "612a8492",
      "metadata": {
        "id": "612a8492"
      },
      "outputs": [],
      "source": [
        "def loadModel(outFolder, epoch, modelName='model'):\n",
        "    modelFile = os.path.join(outFolder, modelName + '_Ep' + str(epoch) + '.pt')\n",
        "    model = torch.load(modelFile,weights_only=False)\n",
        "    return model\n",
        "\n",
        "print(\"Load model from \", out)\n",
        "testepoch = 100\n",
        "\n",
        "pretrained_model = loadModel(out, epoch=testepoch)\n",
        "model.inittime = 0\n",
        "model.lstminv =  pretrained_model.lstminv\n",
        "\n",
        "val_z_all =val_z #np.concatenate((train_z, val_z),axis = 1) to use the whole training time period as warmup\n",
        "val_x_all =val_x #np.concatenate((train_x, val_x),axis = 1) to use the whole training time period as warmup\n",
        "\n",
        "val_c_all = np.expand_dims(val_c, axis=1)\n",
        "val_c_all = np.repeat(val_c_all, val_x_all.shape[1], axis=1)\n",
        "\n",
        "zTest = np.concatenate([val_z_all, val_c_all], 2)  # Add attributes to historical forcings for LSTM-parameterization\n",
        "\n",
        "testTuple = (val_x_all, zTest)\n",
        "testbatch =200\n",
        "\n",
        "filePathLst = [out+\"/Qs\",out+\"/Q0\",out+\"/Q1\",out+\"/Q2\",out+\"/ET\"]\n",
        "train.testModel(\n",
        "    model, testTuple, c=None, batchSize=testbatch, filePathLst=filePathLst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348dcdb4",
      "metadata": {
        "id": "348dcdb4"
      },
      "outputs": [],
      "source": [
        "# This first three year is used to warmup model states.\n",
        "warmup = pd.date_range(f'{1986}-10-01', f'{1989}-09-30', freq='d')\n",
        "dataPred = pd.read_csv(  out+\"/Qs\", dtype=np.float32, header=None).values\n",
        "dataPred = np.expand_dims(dataPred, axis=-1)\n",
        "\n",
        "\n",
        "evaDict = [stat.statError(dataPred[:,len(warmup):,0], val_y[:,len(warmup):,0])]\n",
        "evaDictLst = evaDict\n",
        "keyLst = ['NSE', 'KGE','FLV','FHV', 'lowRMSE', 'highRMSE']\n",
        "dataBox = list()\n",
        "for iS in range(len(keyLst)):\n",
        "    statStr = keyLst[iS]\n",
        "    temp = list()\n",
        "    for k in range(len(evaDictLst)):\n",
        "        data = evaDictLst[k][statStr]\n",
        "        #data = data[~np.isnan(data)]\n",
        "        temp.append(data)\n",
        "    dataBox.append(temp)\n",
        "\n",
        "\n",
        "print(\"dHBV model'NSE', 'KGE','absFLV','absFHV', 'lowRMSE', 'highRMSE'\",\n",
        "      np.nanmedian(dataBox[0][0]),\n",
        "      np.nanmedian(dataBox[1][0]), np.nanmedian(dataBox[2][0]), np.nanmedian(dataBox[3][0]),\n",
        "      np.nanmedian(dataBox[4][0]), np.nanmedian(dataBox[5][0]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}